{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from collections import namedtuple, defaultdict, Counter\n",
    "from itertools import groupby, chain, count\n",
    "from operator import itemgetter\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = '/home/mikhail/bioinformatics/data/sec_struct'\n",
    "DATA_DIR = '/home/mikhail/bioinformatics/data/fragment_data_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(pdb_id):\n",
    "    with open(os.path.join(MODELS_DIR, '{}.pickle'.format(pdb_id)), 'rb') as infile:\n",
    "        return pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/mikhail/bioinformatics/data/nonredundant.txt', 'r') as infile:\n",
    "    nonredundant_chain_ids = {tuple(chain_id.split('.cif1_')) for chain_id in infile.read().splitlines()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "urs_models = {pdb_id: load_model(pdb_id) for pdb_id, _ in nonredundant_chain_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fragment = namedtuple('Fragment', 'pdb_id ch type index global_index members')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fragment_chains(chain_ids):\n",
    "    fragment_chains = []\n",
    "    \n",
    "    for pdb_id, chain_id in chain_ids:\n",
    "        fragment_id_res = []\n",
    "        \n",
    "        for res in urs_models[pdb_id].chains[chain_id]['RES']:\n",
    "            assert sum(res[key] is not None for key in ['WING', 'THREAD']) == 1\n",
    "            type_ = 'WING' if res['THREAD'] is None else 'THREAD'\n",
    "            fragment_id_res.append(((type_, res[type_]), res))\n",
    "        \n",
    "        fragment_chains.append([\n",
    "            Fragment(pdb_id, chain_id, type_, index, global_index, [res for _, res in group]) \n",
    "            for global_index, ((type_, index), group) in enumerate(groupby(fragment_id_res, itemgetter(0)))])\n",
    "    \n",
    "    return fragment_chains\n",
    "\n",
    "\n",
    "def print_fragment_chain(fragment_chain):\n",
    "    print('meta:', fragment_chain[0].pdb_id, fragment_chain[0].ch)\n",
    "    for fragment in fragment_chain:\n",
    "        print(fragment.type, fragment.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coords(res):\n",
    "    return [[atom[key] for key in 'XYZ'] for atom in res['ATOMS']]\n",
    "\n",
    "\n",
    "def get_fragment_atoms(fragment):\n",
    "    return list(chain.from_iterable([get_coords(res) for res in fragment.members]))\n",
    "\n",
    "\n",
    "def fragment_distance(left, right):\n",
    "    return np.min(cdist(get_fragment_atoms(left), get_fragment_atoms(right), 'euclidean'))\n",
    "\n",
    "\n",
    "def fragment_sequence_distance(left, right):\n",
    "    indices = [[urs_models[fragment.pdb_id].dssrnucls[res['DSSR']][2] \n",
    "                for res in fragment.members] \n",
    "               for fragment in [left, right]]\n",
    "    \n",
    "    return min(abs(min(indices[r]) - max(indices[l])) for l, r in [(0, 1), (1, 0)])\n",
    "\n",
    "\n",
    "def fragment_relation(left, right):\n",
    "    if left.type == 'WING' and right.type == 'WING' and fragment_sequence_distance(left, right) == 1:\n",
    "        return 'LC'\n",
    "    \n",
    "    return urs_models[left.pdb_id].NuclRelation(left.members[0]['DSSR'], right.members[0]['DSSR'])\n",
    "\n",
    "\n",
    "def fragment_type(fragment):\n",
    "    types = urs_models[fragment.pdb_id].NuclSS(fragment.members[0]['DSSR']).split(';')\n",
    "    \n",
    "    if types == ['S']:\n",
    "        return [{'S'}, {}]\n",
    "    else:\n",
    "        return [{item[i] for item in types} for i in range(2)]\n",
    "\n",
    "    \n",
    "def fragment_length(fragment):\n",
    "    return len(fragment.members)\n",
    "\n",
    "\n",
    "def fragment_sequence(fragment, fill_length):\n",
    "    sequence = []\n",
    "    \n",
    "    for res in fragment.members:\n",
    "        name = res['NAME']\n",
    "        if len(name) > 1 or name not in 'AUGC':\n",
    "            name = 'M'\n",
    "        \n",
    "        sequence.append(name)\n",
    "    \n",
    "    while len(sequence) < fill_length:\n",
    "        sequence.append('N')\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "\n",
    "def fragment_fragment_distance(left, right):\n",
    "    return right.global_index - left.global_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fragment_chains = get_fragment_chains(nonredundant_chain_ids)\n",
    "len(fragment_chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_lengths = {'{}_{}'.format(fragment_chain[0].pdb_id, fragment_chain[0].ch): len(fragment_chain)\n",
    "                 for fragment_chain in fragment_chains}\n",
    "\n",
    "with open('chain_lengths.pickle', 'wb') as outfile:\n",
    "    pickle.dump(chain_lengths, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3194"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fragments = list(chain.from_iterable(fragment_chains))\n",
    "len(fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что для LR пар фрагментов, таких что пара не (WING, WING), расстояние по сиквенсу между фрагментами не менее 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fragment_chain in fragment_chains:\n",
    "    for i, left in enumerate(fragment_chain):\n",
    "        for right in fragment_chain[i + 1:]:\n",
    "            rel = urs_models[left.pdb_id].NuclRelation(left.members[0]['DSSR'], right.members[0]['DSSR'])\n",
    "            \n",
    "            if rel == 'LR' and {left.type, right.type} != {'WING'}:\n",
    "                assert not fragment_sequence_distance(left, right) <= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем количество фрагментов с MISS нуклеотидами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([any(res['MISS'] for res in fragment.members) for fragment in fragments])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([all(res['MISS'] for res in fragment.members) for fragment in fragments])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как таких фрагментов мало, просто выкинем их из выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем среднее евклидово расстояние между нуклеотидами в цепочках в зависимости от расстояния между нуклеотидами по сиквенсу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distances_on_sequence(sequence_distance, chain_ids):\n",
    "    distances = []\n",
    "    \n",
    "    for pdb_id, chain_id in chain_ids:\n",
    "        atoms = [get_coords(res) for res in urs_models[pdb_id].chains[chain_id]['RES']]\n",
    "\n",
    "        for left_i, left in enumerate(atoms):\n",
    "            right_i = left_i + sequence_distance\n",
    "            if right_i < len(atoms):\n",
    "                right = atoms[right_i]\n",
    "\n",
    "                if len(right) > 0 and len(left) > 0:\n",
    "                    distances.append(np.min(cdist(left, right, 'euclidean')))\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.6089178747660788 0.18143464158571168\n",
      "2 5.805022637977844 1.139063344573624\n",
      "3 8.826294468867898 2.1702721456212735\n",
      "4 11.375384871050775 3.0911026326345694\n",
      "5 13.580455084587362 3.9571849052277495\n",
      "6 15.464713741164214 4.795388865068102\n",
      "7 17.078913211242185 5.581391402615642\n",
      "8 18.510487217879657 6.331259718240907\n",
      "9 19.8926131849052 7.0879635534351415\n",
      "10 21.27094212767807 7.852585143333483\n",
      "11 22.634894209667436 8.647659194179155\n",
      "12 23.986928225656513 9.455061807827107\n",
      "13 25.30002633220102 10.2850526545864\n",
      "14 26.548126949172993 11.102171647714874\n",
      "15 27.701893580894154 11.898910820453164\n",
      "16 28.778787287934346 12.640643528946892\n",
      "17 29.777659563660933 13.32678626005696\n",
      "18 30.6903085631811 13.964265321218692\n",
      "19 31.51841637571828 14.582341197661746\n",
      "20 32.28292385420625 15.173370705588935\n",
      "21 33.008679956457186 15.753404230238889\n",
      "22 33.713414484486165 16.31071002194597\n",
      "23 34.410222311584924 16.8511306588473\n",
      "24 35.09236218716058 17.38296834350296\n",
      "25 35.75480877975653 17.913647820995998\n",
      "26 36.38506302503171 18.44954415401773\n",
      "27 36.99117387725448 18.981030201116575\n",
      "28 37.574894501909434 19.49186384537123\n",
      "29 38.14497637919219 19.96817509022729\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 30):\n",
    "    distances = get_distances_on_sequence(i, nonredundant_chain_ids)\n",
    "    mean = np.mean(distances)\n",
    "    std = np.std(distances)\n",
    "    \n",
    "    print(i, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.6260765664629682\n",
      "2 7.785276681017853\n",
      "3 13.03325262549608\n",
      "4 17.807200622220197\n",
      "5 22.11585447139675\n",
      "6 26.0258733571037\n",
      "7 29.683053987081585\n",
      "8 33.02704390344376\n",
      "9 36.30172731427528\n",
      "10 39.25754657387546\n",
      "11 41.81045661075709\n",
      "12 44.10900386542413\n",
      "13 45.99871032974728\n",
      "14 47.97641897849397\n",
      "15 50.27048322823245\n",
      "16 52.605720687012735\n",
      "17 54.74848725764026\n",
      "18 57.127149955165805\n",
      "19 59.54524326090204\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 20):\n",
    "    distances = list(sorted(get_distances_on_sequence(i, nonredundant_chain_ids)))\n",
    "    print(i, distances[-100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что по-хорошему, если ставить positive threshold в 8 ангстрем, то нужно брать выборку из фрагментов на расстоянии не менее 10, чтобы ловить реально LR взаимодействия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим теперь на расстояния по сиквенсу в LR фрагментах "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:04<00:00, 10.88it/s]\n"
     ]
    }
   ],
   "source": [
    "lr_sequence_distances = []\n",
    "\n",
    "for fragment_chain in tqdm(fragment_chains):\n",
    "    for i, left in enumerate(fragment_chain):\n",
    "        for right in fragment_chain[i + 1:]:\n",
    "            if fragment_relation(left, right) == 'LR':\n",
    "                lr_sequence_distances.append(fragment_sequence_distance(left, right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "701042"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lr_sequence_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7376"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter(lr_sequence_distances)\n",
    "\n",
    "sum([counter[d] for d in range(2, 15)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что пар на малом расстоянии мало, не будем их включать в выборку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь самая проблемная часть: сиквенсы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 617),\n",
       " (3, 615),\n",
       " (4, 570),\n",
       " (5, 356),\n",
       " (1, 289),\n",
       " (6, 257),\n",
       " (7, 181),\n",
       " (8, 93),\n",
       " (9, 66),\n",
       " (10, 39),\n",
       " (11, 27),\n",
       " (12, 23),\n",
       " (13, 15),\n",
       " (15, 9),\n",
       " (14, 6),\n",
       " (20, 5),\n",
       " (16, 4),\n",
       " (19, 3),\n",
       " (22, 3),\n",
       " (17, 3),\n",
       " (18, 2),\n",
       " (21, 1),\n",
       " (28, 1),\n",
       " (23, 1),\n",
       " (48, 1),\n",
       " (47, 1),\n",
       " (25, 1),\n",
       " (122, 1),\n",
       " (37, 1),\n",
       " (26, 1),\n",
       " (31, 1),\n",
       " (35, 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([fragment_length(fragment) for fragment in fragments]).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что фрагментов длинее 10 мало. Предлагается пока их просто выкинуть, а не обрезать, потому что скорее всего у них какая-то отличная от коротких геометрия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "FragmentData = namedtuple('FragmentData', 'type sequence length')\n",
    "FragmentPairData = namedtuple('FragmentPairData', 'lefts rights relation fragment_distance sequence_distance')\n",
    "\n",
    "FragmentPair = namedtuple('FragmentPair', 'data distance pdb_id chain_id left right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fragment(fragment, fragment_length_threshold):\n",
    "    return all([\n",
    "        not any(res['MISS'] for res in fragment.members),\n",
    "        fragment_length(fragment) <= fragment_length_threshold\n",
    "    ])\n",
    "\n",
    "\n",
    "def check_fragment_pair(left, right, fragment_sequence_distance_threshold):\n",
    "    return all([\n",
    "        fragment_relation(left, right) == 'LR',\n",
    "        fragment_sequence_distance(left, right) >= fragment_sequence_distance_threshold\n",
    "    ])\n",
    "\n",
    "\n",
    "def make_fragment_data(fragment, fragment_length_threshold):\n",
    "    return FragmentData(\n",
    "        type=fragment_type(fragment), \n",
    "        sequence=fragment_sequence(fragment, fragment_length_threshold), \n",
    "        length=fragment_length(fragment))\n",
    "\n",
    "\n",
    "def make_fragment_pair_data(lefts, rights, fragment_length_threshold):\n",
    "    center = len(lefts) // 2\n",
    "    \n",
    "    return FragmentPairData(\n",
    "        lefts=[make_fragment_data(item, fragment_length_threshold) for item in lefts],\n",
    "        rights=[make_fragment_data(item, fragment_length_threshold) for item in rights],\n",
    "        relation=[[fragment_relation(l, r) for l in lefts] for r in rights], \n",
    "        fragment_distance=fragment_fragment_distance(lefts[center], rights[center]),\n",
    "        sequence_distance=fragment_sequence_distance(lefts[center], rights[center]))\n",
    "\n",
    "\n",
    "def make_data(fragment_chains, fragment_length_threshold, fragment_sequence_distance_threshold, num_neighbors):\n",
    "    data = []\n",
    "    \n",
    "    for fragment_chain in tqdm(fragment_chains):\n",
    "        for i, left in enumerate(fragment_chain):\n",
    "            for j, right in enumerate(fragment_chain):\n",
    "                ij_ok = all(num_neighbors <= index < len(fragment_chain) - num_neighbors for index in [i, j])\n",
    "                if i != j and ij_ok:\n",
    "                    lefts, rights = [fragment_chain[index - num_neighbors:index + num_neighbors + 1] \n",
    "                                     for index in [i, j]]\n",
    "                    fragments_ok = all(check_fragment(fragment, fragment_length_threshold) \n",
    "                                       for fragment in chain.from_iterable([lefts, rights]))\n",
    "                    fragment_pairs_ok = check_fragment_pair(left, right, fragment_sequence_distance_threshold)\n",
    "                    \n",
    "                    if fragments_ok and fragment_pairs_ok:\n",
    "                        data.append(FragmentPair(\n",
    "                            data=make_fragment_pair_data(lefts, rights, fragment_length_threshold),\n",
    "                            distance=fragment_distance(left, right),\n",
    "                            pdb_id=left.pdb_id, \n",
    "                            chain_id=left.ch, \n",
    "                            left=i, \n",
    "                            right=j))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [04:36<00:00,  6.28s/it]\n"
     ]
    }
   ],
   "source": [
    "data = make_data(fragment_chains, 10, 15, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('2qwy', 'C'): 32,\n",
       "         ('4wfl', 'A'): 368,\n",
       "         ('4rge', 'C'): 110,\n",
       "         ('6dtd', 'C'): 2,\n",
       "         ('1f1t', 'A'): 4,\n",
       "         ('3rw6', 'H'): 66,\n",
       "         ('4enc', 'A'): 24,\n",
       "         ('4rmo', 'B'): 2,\n",
       "         ('5x2g', 'B'): 14,\n",
       "         ('6qzp', 'L5'): 446592,\n",
       "         ('1kh6', 'A'): 4,\n",
       "         ('4jf2', 'A'): 68,\n",
       "         ('4y4o', '2A'): 328676,\n",
       "         ('1l9a', 'B'): 392,\n",
       "         ('6qzp', 'L8'): 760,\n",
       "         ('6dlr', 'A'): 506,\n",
       "         ('4yaz', 'R'): 266,\n",
       "         ('3la5', 'A'): 108,\n",
       "         ('3pdr', 'X'): 962,\n",
       "         ('5fjc', 'A'): 310,\n",
       "         ('5u3g', 'B'): 148,\n",
       "         ('4qlm', 'A'): 104,\n",
       "         ('2z75', 'B'): 326,\n",
       "         ('4far', 'A'): 4484,\n",
       "         ('3k1v', 'A'): 2,\n",
       "         ('4y4o', '1a'): 110206,\n",
       "         ('4frg', 'B'): 180,\n",
       "         ('4prf', 'B'): 64,\n",
       "         ('4p95', 'A'): 1400,\n",
       "         ('3f2q', 'X'): 316,\n",
       "         ('3e5c', 'A'): 40,\n",
       "         ('1u9s', 'A'): 1014,\n",
       "         ('5kpy', 'A'): 20,\n",
       "         ('4lvw', 'A'): 168,\n",
       "         ('3npq', 'A'): 2,\n",
       "         ('3hhn', 'E'): 298,\n",
       "         ('3d2v', 'A'): 150,\n",
       "         ('6qzp', 'S2'): 133166,\n",
       "         ('5k7d', 'A'): 14,\n",
       "         ('6fz0', 'A'): 4,\n",
       "         ('6qzp', 'L7'): 424,\n",
       "         ('5btp', 'B'): 32,\n",
       "         ('2r8s', 'R'): 978})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([(item.pdb_id, item.chain_id) for item in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как теперь у нас нет верхней границы на расстояние по сиквенсу для пар, то получили сильный дисбаланс по цепочкам и pdb_id. В цепочках ('6qzp', 'L5'), ('4y4o', '2A'), ('6qzp', 'S2'), ('4y4o', '1a') лежат почти все пары. Из-за этого получаются проблемы с кроссвалидацией по количеству фолдов, превышающему 2, если мы хотим чтобы все пары одной цепочки попадали в один и тот же фолд."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09967647511939609"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([item.distance < 8 for item in data if item.pdb_id not in ['4y4o', '6qzp']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008621173773196515"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([item.distance < 8 for item in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дисбаланс в коротких цепочках тоже ожидаемо меньше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним датасет, потом преобразуем в матрицу для random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_index, i in enumerate(range(0, len(data), BATCH_SIZE)):\n",
    "    with open(os.path.join(DATA_DIR, 'batch_{}'.format(batch_index)), 'wb') as outfile:\n",
    "        pickle.dump(data[i:i + BATCH_SIZE], outfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    for i in count():\n",
    "        fpath = os.path.join(DATA_DIR, 'batch_{}'.format(i))\n",
    "        \n",
    "        if os.path.exists(fpath):\n",
    "            with open(fpath, 'rb') as infile:\n",
    "                yield from pickle.load(infile)\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FragmentPair(data=FragmentPairData(lefts=[FragmentData(type=[{'H'}, {'I'}], sequence=['G', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N'], length=1), FragmentData(type=[{'S'}, {}], sequence=['C', 'G', 'C', 'G', 'G', 'C', 'N', 'N', 'N', 'N'], length=6), FragmentData(type=[{'H'}, {'P'}], sequence=['G', 'A', 'U', 'U', 'U', 'A', 'A', 'N', 'N', 'N'], length=7)], rights=[FragmentData(type=[{'S'}, {}], sequence=['U', 'U', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N'], length=2), FragmentData(type=[{'S'}, {}], sequence=['G', 'C', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N'], length=2), FragmentData(type=[{'H'}, {'P'}], sequence=['A', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N'], length=1)], relation=[['LR', 'LR', 'LC'], ['LR', 'LR', 'LC'], ['LR', 'LC', 'SM']], fragment_distance=5, sequence_distance=15), distance=7.462208118244893, pdb_id='2qwy', chain_id='C', left=1, right=6)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fragment_pair = next(generate_data())\n",
    "fragment_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractor:\n",
    "    def extract(self, fragment_pair):\n",
    "        pass\n",
    "        \n",
    "    def describe(self, fragment_pair):\n",
    "        pass\n",
    "\n",
    "\n",
    "def extract(fragment_pair, extractors, method='extract'):\n",
    "    return list(chain.from_iterable([getattr(extractor, method)(fragment_pair) for extractor in extractors]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(items, possible_items):\n",
    "    result = [0] * len(possible_items)\n",
    "    for item in items:\n",
    "        result[possible_items.index(item)] = 1\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "class FragmentExtractor(Extractor):\n",
    "    BASES = 'AUGCMN'\n",
    "    FIRST_TYPES = 'SHBIJ'\n",
    "    SECOND_TYPES = 'CIP'\n",
    "    \n",
    "    def extract(self, fragment_pair):\n",
    "        result = []\n",
    "        \n",
    "        for fragment_data in chain.from_iterable([fragment_pair.data.lefts, fragment_pair.data.rights]):\n",
    "            result.append(fragment_data.length)\n",
    "            \n",
    "            for base in fragment_data.sequence:\n",
    "                result.extend(onehot([base], self.BASES))\n",
    "            \n",
    "            first, second = fragment_data.type\n",
    "                \n",
    "            result.extend(onehot(first, self.FIRST_TYPES))\n",
    "            result.extend(onehot(second, self.SECOND_TYPES))\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def describe(self, fragment_pair):\n",
    "        result = []\n",
    "        \n",
    "        for lr in ['lefts', 'rights']:\n",
    "            for fragment_index, fragment_data in enumerate(fragment_pair.data.lefts):\n",
    "                result.append('sequence {}[{}] length'.format(lr, fragment_index))\n",
    "                \n",
    "                for sequence_index, _ in enumerate(fragment_data.sequence):\n",
    "                    for base in self.BASES:\n",
    "                        result.append('sequence {}[{}][{}] == {}'.format(lr, fragment_index, sequence_index, base))\n",
    "                \n",
    "                result.extend(['fragment type {}[{}] for first {}'.format(lr, fragment_index, letter) \n",
    "                               for letter in self.FIRST_TYPES])\n",
    "                result.extend(['fragment type {}[{}] for second {}'.format(lr, fragment_index, letter) \n",
    "                               for letter in self.SECOND_TYPES])\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "class TinyExtractor(Extractor):\n",
    "    def extract(self, fragment_pair):\n",
    "        return [fragment_pair.data.fragment_distance, fragment_pair.data.sequence_distance]\n",
    "    \n",
    "    def describe(self, fragment_pair):\n",
    "        return ['fragment_distance', 'sequence_distance']\n",
    "\n",
    "\n",
    "class RelationExtractor(Extractor):\n",
    "    RELATIONS = ['SM', 'LC', 'LR']\n",
    "    \n",
    "    def extract(self, fragment_pair):\n",
    "        result = []\n",
    "        \n",
    "        for rel in chain.from_iterable(fragment_pair.data.relation):\n",
    "            result.extend(onehot([rel], self.RELATIONS))\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def describe(self, fragment_pair):\n",
    "        result = []\n",
    "        \n",
    "        for i, rels in enumerate(fragment_pair.data.relation):\n",
    "            for j, _ in enumerate(rels):\n",
    "                for rel_type in self.RELATIONS:\n",
    "                    result.append('rel_matrix[{}][{}] == {}'.format(i, j, rel_type))\n",
    "                \n",
    "        return result\n",
    "\n",
    "\n",
    "class BowFragmentExtractor(Extractor):\n",
    "    BASES = 'AUGCMN'\n",
    "    FIRST_TYPES = 'SHBIJ'\n",
    "    SECOND_TYPES = 'CIP'\n",
    "    \n",
    "    def extract(self, fragment_pair):\n",
    "        result = []\n",
    "        \n",
    "        for fragment_data in chain.from_iterable([fragment_pair.data.lefts, fragment_pair.data.rights]):\n",
    "            result.append(fragment_data.length)\n",
    "            \n",
    "            bow = [0] * len(self.BASES)\n",
    "            for base in fragment_data.sequence:\n",
    "                bow[self.BASES.index(base)] += 1\n",
    "            result.extend(bow)\n",
    "            \n",
    "            first, second = fragment_data.type\n",
    "            first_onehot, second_onehot = [0] * len(self.FIRST_TYPES), [0] * len(self.SECOND_TYPES)\n",
    "            \n",
    "            for item in first:\n",
    "                first_onehot[self.FIRST_TYPES.index(item)] = 1\n",
    "                \n",
    "            for item in second:\n",
    "                second_onehot[self.SECOND_TYPES.index(item)] = 1\n",
    "                \n",
    "            result.extend(first_onehot)\n",
    "            result.extend(second_onehot)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def describe(self, fragment_pair):\n",
    "        result = []\n",
    "        \n",
    "        for lr in ['lefts', 'rights']:\n",
    "            for fragment_index, _ in enumerate(fragment_pair.data.lefts):\n",
    "                result.append('sequence {}[{}] length'.format(lr, fragment_index))\n",
    "                for base in self.BASES:\n",
    "                    result.append('sequence {}[{}] count of {}'.format(lr, fragment_index, base))\n",
    "                \n",
    "                result.extend(['fragment type {}[{}] for first {}'.format(lr, fragment_index, letter) \n",
    "                               for letter in self.FIRST_TYPES])\n",
    "                result.extend(['fragment type {}[{}] for second {}'.format(lr, fragment_index, letter) \n",
    "                               for letter in self.SECOND_TYPES])\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "class BowRelationExtractor(Extractor):\n",
    "    RELATIONS = ['SM', 'LC', 'LR']\n",
    "    \n",
    "    def extract(self, fragment_pair):\n",
    "        bow = [0] * 3\n",
    "        for rel in chain.from_iterable(fragment_pair.data.relation):\n",
    "            bow[self.RELATIONS.index(rel)] += 1\n",
    "        \n",
    "        return bow\n",
    "    \n",
    "    def describe(self, fragment_pair):\n",
    "        return ['relation count {}'.format(rel) for rel in self.RELATIONS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractors = [BowFragmentExtractor(), TinyExtractor(), BowRelationExtractor()]\n",
    "description = extract(fragment_pair, extractors, method='describe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1032806it [00:54, 18901.12it/s]\n"
     ]
    }
   ],
   "source": [
    "digitized_features = [extract(fragment_pair, extractors) for fragment_pair in tqdm(generate_data())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1032806it [00:39, 26441.88it/s]\n"
     ]
    }
   ],
   "source": [
    "target_distance = []\n",
    "pdb_ids = []\n",
    "chain_ids = []\n",
    "\n",
    "for fragment_pair in tqdm(generate_data()):\n",
    "    target_distance.append(fragment_pair.distance)\n",
    "    pdb_ids.append(fragment_pair.pdb_id)\n",
    "    chain_ids.append(fragment_pair.chain_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitized_features = np.array(digitized_features, dtype=np.int16)\n",
    "target_distance = np.array(target_distance)\n",
    "pdb_ids = np.array(pdb_ids)\n",
    "chain_ids = np.array(chain_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(digitized_features, target_distance, pdb_ids, chain_ids, description, prefix=''):\n",
    "    np.save(os.path.join(DATA_DIR, '{}_features.npy'.format(prefix)), digitized_features)\n",
    "    np.save(os.path.join(DATA_DIR, '{}_distance.npy'.format(prefix)), target_distance)\n",
    "    np.save(os.path.join(DATA_DIR, '{}_pdb_ids.npy'.format(prefix)), pdb_ids)\n",
    "    np.save(os.path.join(DATA_DIR, '{}_chain_ids.npy'.format(prefix)), chain_ids)\n",
    "    \n",
    "    with open(os.path.join(DATA_DIR, '{}_description.pickle'.format(prefix)), 'wb') as outfile:\n",
    "        pickle.dump(description, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(digitized_features, target_distance, pdb_ids, chain_ids, description, prefix='bow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
