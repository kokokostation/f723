{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import dill\n",
    "import numpy as np\n",
    "import multiprocessing_on_dill as mp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from cityhash import CityHash64\n",
    "from itertools import groupby\n",
    "\n",
    "from f723.tools.urs.extraction import assemble_chains, get_sec_struct_model\n",
    "from f723.tools.dataset.entities import NucleotideFeatures, PairFeatures, PairMeta, PairData, make_pair, Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = '/home/mikhail/bioinformatics/data/dataset_all_60'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(index):\n",
    "    with open(os.path.join(DATASET_DIR, 'batch_{}'.format(index)), 'rb') as infile:\n",
    "        return dill.load(infile)\n",
    "    \n",
    "\n",
    "def get_data():\n",
    "    return chain.from_iterable((get_batch(i) for i in tqdm(range(30))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим к фичам ещё расстояние между нуклеотидами пары в смысле индексов нуклеотидов в цепи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesExtractor:\n",
    "    def extract(self, item):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def description(self, item):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class NucleotideFeaturesExtractor(FeaturesExtractor):\n",
    "    SECONDARY_STRUCTURES = ['BC', 'BI', 'BP', 'HC', 'HI', 'HP', 'IC', 'II', 'IP', 'JC', 'JI', 'JP', 'S']\n",
    "    BASES = ['a', 'u', 'g', 'c']\n",
    "    NUCLEOTIDE_FEATURES_LEN = len(SECONDARY_STRUCTURES) + len(BASES) + 1 + 2\n",
    "    \n",
    "    def extract(self, nf):\n",
    "        if nf is None:\n",
    "            features = [0] * (self.NUCLEOTIDE_FEATURES_LEN + 1)\n",
    "        else:\n",
    "            features = [1]\n",
    "\n",
    "            nucleotide_secondary_structures = nf.secondary_structure.split(';')\n",
    "            for secondary_structure in self.SECONDARY_STRUCTURES:\n",
    "                features.append(int(secondary_structure in nucleotide_secondary_structures))\n",
    "\n",
    "            for base in self.BASES:\n",
    "                features.append(int(nf.base == base))\n",
    "\n",
    "            features.append(int(nf.base in self.BASES))\n",
    "\n",
    "            features.extend([nf.fragment_length, nf.fragment_index])\n",
    "\n",
    "        return features\n",
    "    \n",
    "    def description(self, nf):\n",
    "        meta_features = [('nucleotide', 'is_dummy')]\n",
    "        secondary_structure_features = [('secondary_structure', typ) for typ in self.SECONDARY_STRUCTURES]\n",
    "        base_features = [('base', base) for base in self.BASES] + [('base', 'rare')]\n",
    "        fragment_features = [('fragment', 'length'), ('fragment', 'index')]\n",
    "\n",
    "        return meta_features + secondary_structure_features + base_features + fragment_features\n",
    "\n",
    "\n",
    "class RelationFeaturesExtractor(FeaturesExtractor):\n",
    "    RELATIONS = ['LC', 'LR', 'SM']\n",
    "    \n",
    "    def extract(self, pair):\n",
    "        relation = [int(pair.features.relation == relation) for relation in self.RELATIONS] \n",
    "        distance = [abs(pair.meta.pair.nt_left.index - pair.meta.pair.nt_right.index)]\n",
    "        \n",
    "        return relation + distance\n",
    "    \n",
    "    def description(self, pair):\n",
    "        return [('relation', relation) for relation in self.RELATIONS] + [('distance',)]\n",
    "    \n",
    "\n",
    "class PairFeaturesExtractor(FeaturesExtractor):\n",
    "    def __init__(self):\n",
    "        self._nucleotide_features_extractor = NucleotideFeaturesExtractor()\n",
    "        self._relation_features_extractor = RelationFeaturesExtractor()\n",
    "    \n",
    "    def extract(self, pair):\n",
    "        result_features = []\n",
    "        neighbour_sets = [pair.features.neighbours_left, pair.features.neighbours_right]\n",
    "\n",
    "        for neighbour_set_permutation in [neighbour_sets, reversed(neighbour_sets)]:\n",
    "            features = []\n",
    "            \n",
    "            for nf in chain.from_iterable(neighbour_set_permutation):\n",
    "                features.extend(self._nucleotide_features_extractor.extract(nf))\n",
    "            features.extend(self._relation_features_extractor.extract(pair))\n",
    "            \n",
    "            result_features.append(features)\n",
    "        \n",
    "        return result_features\n",
    "    \n",
    "    def description(self, pair):\n",
    "        features_description = []\n",
    "        \n",
    "        for lr, nfs in [('left', pair.features.neighbours_left), \n",
    "                        ('right', pair.features.neighbours_right)]:\n",
    "            for index, nf in enumerate(nfs):\n",
    "                for feature_description in self._nucleotide_features_extractor.description(pair):\n",
    "                    features_description.append((lr, index - (len(nfs) - 1) // 2, feature_description))\n",
    "        \n",
    "        features_description.extend(self._relation_features_extractor.description(pair))\n",
    "        \n",
    "        return features_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_PATH = os.path.join(DATASET_DIR, 'features_with_distance.npy')\n",
    "TARGET_PATH = os.path.join(DATASET_DIR, 'target_with_distance.npy')\n",
    "GROUPS_PATH = os.path.join(DATASET_DIR, 'groups_with_distance.npy')\n",
    "\n",
    "PAIR_TYPES = ['ss_bps', 'noncanonical_bps', 'random']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [02:45<00:00,  2.42s/it]\n"
     ]
    }
   ],
   "source": [
    "pair_sample = get_batch(0)[0]\n",
    "FEATURES_SHAPE = (2 * sum(1 for _ in get_data()), \n",
    "                  len(PairFeaturesExtractor().description(pair_sample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features():\n",
    "    pair_features_extractor = PairFeaturesExtractor()\n",
    "    pair_sample = get_batch(0)[0]\n",
    "    features = np.memmap(FEATURES_PATH, shape=FEATURES_SHAPE, mode='w+')\n",
    "    \n",
    "    index = 0\n",
    "    for pair_data in get_data():\n",
    "        for pair_features in pair_features_extractor.extract(pair_data):\n",
    "            features[index] = pair_features\n",
    "            \n",
    "            index += 1\n",
    "            \n",
    "    target = np.repeat([PAIR_TYPES.index(pair_data.meta.type) for pair_data in get_data()], 2)\n",
    "    pdb_ids = np.repeat([pair_data.meta.pdb_id for pair_data in get_data()], 2)\n",
    "    \n",
    "    np.save(TARGET_PATH, target)\n",
    "    np.save(GROUPS_PATH, pdb_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [14:33<00:00, 22.99s/it]\n",
      "100%|██████████| 30/30 [01:30<00:00,  2.54s/it]\n",
      "100%|██████████| 30/30 [01:29<00:00,  2.50s/it]\n"
     ]
    }
   ],
   "source": [
    "prepare_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features():\n",
    "    features = np.memmap(FEATURES_PATH, shape=FEATURES_SHAPE)\n",
    "    target = np.load(TARGET_PATH)\n",
    "    pdb_ids = np.load(GROUPS_PATH)\n",
    "    \n",
    "    return features, target, pdb_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, target, pdb_ids = load_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = target != 0  # throw away secondary structure pairs\n",
    "\n",
    "features = features[mask]\n",
    "target = target[mask]\n",
    "pdb_ids = pdb_ids[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train positive rate: 0.0026536722659665614, test positive rate: 0.002601588276253125\n",
      "(array([0.89151599, 0.99833828]), array([0.36194241, 0.99988512]), array([0.51485944, 0.9991111 ]), array([   3542, 1357934]))\n"
     ]
    }
   ],
   "source": [
    "group_kfold = GroupKFold(n_splits=5)\n",
    "group_kfold.get_n_splits(features, target, pdb_ids)\n",
    "feature_importances = []\n",
    "\n",
    "for train_index, test_index in group_kfold.split(features, target, pdb_ids):\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    print('Train positive rate: {}, test positive rate: {}'.format(\n",
    "        np.mean(y_train == 1), np.mean(y_test == 1)))\n",
    "    \n",
    "    model = RandomForestClassifier(class_weight='balanced', n_estimators=100, n_jobs=8)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    feature_importances.append(model.feature_importances_)\n",
    "    \n",
    "    print(precision_recall_fscore_support(y_test, y_pred))\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Никакой разницы эта фича не делает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = PairFeaturesExtractor().description(pair_sample)\n",
    "feature_importances = np.array(feature_importances)\n",
    "mean_feature_importances = feature_importances.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('right', -3, ('fragment', 'index')), 0.005200427607819375),\n",
       " (('left', 0, ('fragment', 'index')), 0.005250879806340551),\n",
       " (('right', -4, ('fragment', 'length')), 0.0054191361016154346),\n",
       " (('right', 0, ('fragment', 'index')), 0.0054558460694541155),\n",
       " (('right', 1, ('secondary_structure', 'JC')), 0.005575806803694708),\n",
       " (('right', 1, ('secondary_structure', 'HC')), 0.0056328425208703415),\n",
       " (('right', -3, ('fragment', 'length')), 0.005794128276486197),\n",
       " (('left', -5, ('fragment', 'length')), 0.005854986018202906),\n",
       " (('left', 1, ('secondary_structure', 'IC')), 0.005914806926669246),\n",
       " (('left', -3, ('fragment', 'length')), 0.005981165060443585),\n",
       " (('right', -2, ('fragment', 'length')), 0.006013305962351617),\n",
       " (('right', 5, ('fragment', 'index')), 0.006061284148834449),\n",
       " (('left', -2, ('fragment', 'length')), 0.006325700292497207),\n",
       " (('right', -5, ('fragment', 'length')), 0.006355754244356948),\n",
       " (('right', -1, ('fragment', 'length')), 0.006511810452424899),\n",
       " (('left', 5, ('fragment', 'index')), 0.006551336663773857),\n",
       " (('left', 1, ('secondary_structure', 'JC')), 0.006659370429820951),\n",
       " (('left', 4, ('fragment', 'index')), 0.006829782021226123),\n",
       " (('left', 5, ('fragment', 'length')), 0.006915026430522588),\n",
       " (('left', -1, ('fragment', 'length')), 0.007035069987044542),\n",
       " (('left', 4, ('fragment', 'length')), 0.007131177677163041),\n",
       " (('left', 3, ('fragment', 'length')), 0.007203098509289514),\n",
       " (('right', 4, ('fragment', 'index')), 0.007287701222089401),\n",
       " (('right', 4, ('fragment', 'length')), 0.007451376687698129),\n",
       " (('right', 2, ('secondary_structure', 'S')), 0.007470445318926121),\n",
       " (('right', 5, ('fragment', 'length')), 0.007518427363166468),\n",
       " (('right', 3, ('fragment', 'length')), 0.007526854472194858),\n",
       " (('right', 0, ('secondary_structure', 'S')), 0.0075772393807408975),\n",
       " (('right', 1, ('secondary_structure', 'IC')), 0.007630699470666152),\n",
       " (('left', 1, ('secondary_structure', 'HC')), 0.007809605461417629),\n",
       " (('right', 2, ('fragment', 'length')), 0.008061822777240057),\n",
       " (('right', 0, ('fragment', 'length')), 0.008234532229203605),\n",
       " (('left', 3, ('fragment', 'index')), 0.008278132988565166),\n",
       " (('left', 0, ('fragment', 'length')), 0.00838198033693593),\n",
       " (('left', 2, ('secondary_structure', 'S')), 0.008576355149213535),\n",
       " (('right', 3, ('fragment', 'index')), 0.008742788323000154),\n",
       " (('left', 2, ('fragment', 'length')), 0.009073357220253672),\n",
       " (('left', 0, ('secondary_structure', 'S')), 0.00951672768403987),\n",
       " (('left', 1, ('fragment', 'length')), 0.009590469100312849),\n",
       " (('right', 2, ('fragment', 'index')), 0.010077927215040852),\n",
       " (('left', 2, ('fragment', 'index')), 0.010184546004384397),\n",
       " (('right', 1, ('fragment', 'index')), 0.010502897124897144),\n",
       " (('right', 1, ('fragment', 'length')), 0.010691121533059041),\n",
       " (('left', 1, ('fragment', 'index')), 0.011059986169868162),\n",
       " (('relation', 'LC'), 0.01597046298828121),\n",
       " (('distance',), 0.028582517830746158),\n",
       " (('left', 1, ('secondary_structure', 'S')), 0.036562215977233296),\n",
       " (('right', 1, ('secondary_structure', 'S')), 0.05097213741550871),\n",
       " (('relation', 'SM'), 0.08640392474218495),\n",
       " (('relation', 'LR'), 0.10503306230712227)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(description[i], mean_feature_importances[i]) for i in np.argsort(mean_feature_importances)[-50:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хоть distance и находится в ряду наиболее значимых фичей, попробуем объяснить, почему она не поднимает качество. Мне кажется, дело в том, что distance хорошо помогает отбрасывать большое количество случайных пар на больших расстояниях. Но так как мы работаем с парами на расстоянии не более 60 нуклеотидов, то здесь уже распределение неканонических пар по расстоянию гораздо более похоже на равномерное, а сама по себе фича distance не позволяет отличить неканоническое спаривание от случайной пары. Фактически мы уже использовали главный потенциал distance, когда решили работать с парами на малых расстояниях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потюним RandomForest по гриду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_estimators=50; max_features=10\n",
      "(array([0.91122914, 0.99827888]), array([0.33907397, 0.99991384]), array([0.49423868, 0.99909569]), array([   3542, 1357934]))\n",
      "num_estimators=50; max_features=20\n",
      "(array([0.88590604, 0.99836616]), array([0.37267081, 0.99987481]), array([0.52464229, 0.99911992]), array([   3542, 1357934]))\n",
      "num_estimators=50; max_features=40\n",
      "(array([0.87654321, 0.99838743]), array([0.38085827, 0.99986008]), array([0.53099784, 0.99912321]), array([   3542, 1357934]))\n",
      "num_estimators=50; max_features=80\n",
      "(array([0.87557604, 0.99837348]), array([0.37549407, 0.99986082]), array([0.52558783, 0.99911659]), array([   3542, 1357934]))\n",
      "num_estimators=50; max_features=120\n",
      "(array([0.88420348, 0.99836689]), array([0.37295313, 0.9998726 ]), array([0.52462272, 0.99911918]), array([   3542, 1357934]))\n",
      "num_estimators=100; max_features=10\n",
      "(array([0.90845617, 0.99825686]), array([0.33060418, 0.9999131 ]), array([0.48478576, 0.99908429]), array([   3542, 1357934]))\n",
      "num_estimators=100; max_features=20\n",
      "(array([0.88519022, 0.99835368]), array([0.36787126, 0.99987555]), array([0.51974471, 0.99911403]), array([   3542, 1357934]))\n",
      "num_estimators=100; max_features=40\n",
      "(array([0.87703318, 0.99838669]), array([0.38057595, 0.99986082]), array([0.53081315, 0.99912321]), array([   3542, 1357934]))\n",
      "num_estimators=100; max_features=80\n",
      "(array([0.87708751, 0.998361  ]), array([0.37069452, 0.9998645 ]), array([0.52113515, 0.99911219]), array([   3542, 1357934]))\n",
      "num_estimators=100; max_features=120\n",
      "(array([0.89065744, 0.99834195]), array([0.36335404, 0.99988365]), array([0.51614197, 0.9991122 ]), array([   3542, 1357934]))\n",
      "num_estimators=200; max_features=10\n",
      "(array([0.91192518, 0.99825613]), array([0.33032185, 0.99991679]), array([0.48497409, 0.99908577]), array([   3542, 1357934]))\n",
      "num_estimators=200; max_features=20\n",
      "(array([0.89256198, 0.99834856]), array([0.36589497, 0.99988512]), array([0.51902283, 0.99911625]), array([   3542, 1357934]))\n",
      "num_estimators=200; max_features=40\n",
      "(array([0.88352085, 0.99837716]), array([0.3769057 , 0.99987039]), array([0.52839897, 0.99912322]), array([   3542, 1357934]))\n",
      "num_estimators=200; max_features=80\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8f52f2478a48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         model = RandomForestClassifier(class_weight='balanced', n_estimators=num_estimators, \n\u001b[1;32m      6\u001b[0m                                        n_jobs=8, max_features=max_features)\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/f723/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    328\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 330\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/f723/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/f723/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for num_estimators in [50, 100, 200]:\n",
    "    for max_features in [10, 20, 40, 80, 120]:\n",
    "        print('num_estimators={}; max_features={}'.format(num_estimators, max_features))\n",
    "        \n",
    "        model = RandomForestClassifier(class_weight='balanced', n_estimators=num_estimators, \n",
    "                                       n_jobs=8, max_features=max_features)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "        print(precision_recall_fscore_support(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока результаты не очень: дефолтные параметры дают качество на том же уровне."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
